{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scikit_learn_logo_small.svg](images/Scikit_learn_logo_small.svg)\n",
    "# Scikit-learn Tutorial\n",
    "Scikit-learn is a data analysis package for Python that includes many tools for traditional machine learning (no deep learning, but can work in conjunction with Tensorflow, Keras, etc.). Scikit comes standard in most numerical or data analysis Python distributions (e.g. Anaconda) and provides all of the tools necessary to build a powerful machine learning pipeline (even the data!) and evaluate its performance. The Scikit-learn website provides a great categorization of traditional machine learning and data analysis in general with tutorials on how to use the package as well as more in-depth description of the theory of each tool with references to the papers the methods come from to really understand how and when to use each method. Scikit is open source and each part of the package has a link to its source on GitHub and is a great way to learn how to program Python!\n",
    "\n",
    "[Image Source](https://en.wikipedia.org/wiki/Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normally its preferred to import packages as needed from sklearn rather than the\n",
    "whole category, but for the purposes of this tutorial its easier to import the whole \n",
    "thing.\n",
    "\"\"\"\n",
    "import sklearn.datasets as datasets # note the package name is sklearn NOT scikit-learn\n",
    "import sklearn.tree as tree\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.decomposition as decomposition \n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "* Learn how to get data using scikit\n",
    "* Build a simple classifier\n",
    "* Evaluate the classifier\n",
    "* Optimize hyper-parameters via grid search\n",
    "* Build a composite classifier with Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Getting Data\n",
    "Scikit-learn comes with a few toy datasets to try out different methods, but can also fetch datasets from [OpenML](https://www.openml.org) or load them from certain files (images, LIBSVM). If the data is in .csv, .txt, or similar format, Pandas can be used to read the data into a DataFrame which scikit can directly operate on.\n",
    "### Genbank Splice Dataset\n",
    "Primate DNA can be divided into 2 types of regions, Exons and Introns. Exons are the 'useful bits' of DNA that are used to encode the blueprint of whatever organism it belongs to while Introns are the 'junk' (but might be used in other ways) that exist in between Exon regions. When DNA is used to transcribe proteins, the Intron regions are effectively removed and the Exons are spliced together. This dataset provides a number of DNA sequences and the goal is to classify them as Intron/Exon boundaries (in both directions) or not. See the below figure for an example of the type of boundaries we will be looking for.\n",
    "\n",
    "For a better description of the dataset and to view all of the metrics necessary for working on it, visit its OpenML webpage [here](https://www.openml.org/d/46).\n",
    "![RNA_splicing_reaction.svg](images/RNA_splicing_reaction.svg)\n",
    "[Image Source](https://en.wikipedia.org/wiki/RNA_splicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING: you may get an SSL Certificate error (on Mac, haven't tested on others), if this happens\n",
    "the easiet workaround is to install Anaconda Navigator and run Jupyter Notebook from there.\n",
    "\"\"\"\n",
    "\n",
    "# import dataset from OpenML.org using the builtin scikit function\n",
    "splice = datasets.fetch_openml('splice', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the dataset description from the OpenML website\n",
    "print(splice.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Scikit also includes functionality for preparing data for use by a machine learning model, in this case we use the function to split the data into train and test sets with the defualt split of 75/25. We will use the train and test sets to build and evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = model_selection.train_test_split(splice.data, splice.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more datasets that can be imported to or loaded from scikit-learn, visit the website for the Scikit-learn [dataset loading utilites](https://scikit-learn.org/stable/datasets/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Simple Classifier\n",
    "### Decision Tree\n",
    "Decision trees are machine learning models that create a binary tree of left/right decisions based on the training data. Each node in the tree is given an attribute and a value so that when a data point comes to that node, it will go left if the value of its attribut is less than the node's value or right if not (the order may be switched but it operates the same whether left/right or right/left). The splits are chosen based on how well that split divides the data into each class (using Information Theory ideas). The leaves of the tree are the classes that are most likely if a path is taken from the root to that leaf.\n",
    "\n",
    "![CART_tree_titanic_survivors.png](images/CART_tree_titanic_survivors.png)\n",
    "\n",
    "For a more in-depth description of Decision Trees visit its [Scikit-learn doc](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) or [Wikipedia entry](https://en.wikipedia.org/wiki/Decision_tree_learning).\n",
    "\n",
    "[Image Source](https://en.wikipedia.org/wiki/Decision_tree_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "There are several types of Naive Bayes depending on the type of prediction task\n",
    "for our purposes of non-binary classification, GaussianNB suffices\n",
    "\"\"\"\n",
    "dtc = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of Scikit-learn is how symantically similar all of its methods are. So the functions for one method will likely have the same name in another method (there are exceptions, but all classifiers have at least a fit and a predict method). The methods we will need are `fit` and `predict`. These fit the classifier to the training data and predict the labels for the test data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "There is also a `score` option for most models that gives an overall accuracy measure directly given the test data and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in-depth analysis of the classifier's performance its usually best to look at the metrics of `precision`, `recall` and/or `f1`. These measures are not dependent on the prevalance of a given class like accuracy is. For an understanding of why this is important, consider classifying on data with 100 labels of one class and only 5 of the other. A classifier with 95% accuracy can be easily made by simply always predicting the majority (100 count) class.\n",
    "\n",
    "For more on recall, precision, and f1 visit [this Wikipedia entry](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in print() to \"pretty print\" the output in human readable format\n",
    "print(metrics.classification_report(dtc.predict(test_data), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A neat part of using decision trees is being able to rank feature importance to the classification based on its ability to divide the data into the given classes. This could be useful for deciding what features to focus on when using other classifiers (feature selection) or for better understanding what the cause of the decision being made by the model is (this is heavily relied upon in medical data mining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show feature importance as a heatmap, transforming the data into a rectangle for compactness\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.imshow(np.reshape(dtc.tree_.compute_feature_importances(), (6,10)), cmap='hot')\n",
    "ax.set_title(\"Feature Heatmap\")\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble and Random Forest\n",
    "What could be better than 1 decision tree? 100 Decision trees! This is how a random forest operates in principal, by generating multiple decision trees, each given a subset of data, and using the mode (most frequent) label of each tree as the decision of the entire model. This is one of a few ensemble methods which in general work by training multiple machine learning methods (usually of the same type but not always) and somehow aggregating the results for a better overall model. However, there can be a tendency to overfit (perform poorly on the test data, i.e. not generalize well) among certain ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc.fit(train_data, train_labels)\n",
    "print(metrics.classification_report(rfc.predict(test_data), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "Grid searching is a method for tuning the hyper-parameters of a model (the parameters we pass to the model when creating it, the name hyper-parameter is meant to avoid confusion with models that use parameters internally, such as neural networks). By creating a search grid of different hyper-parameters and the acceptable range of values, the `GridSearchCV` method can choose the parameters that maximize the accuracy of the model using Cross Validation. \n",
    "\n",
    "Cross Validation (CV) is a technique used to evaluate models with multiple train/test splits. In each iteration of CV, 1/kth of the data is held back for testing, where k is the number of total CV iterations, and the rest of the data is used to train the model and the portion of data held back is different each time. The goal of CV is to get an average of model accuracy instead of a single test value that could be skewed if the test data was chosen poorly. This helps to choose a more generalizable model.\n",
    "![K-fold_cross_validation_EN.jpg](K-fold_cross_validation_EN.jpg)\n",
    "[Image Source](https://en.wikipedia.org/wiki/Cross-validation_(statistics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'n_estimators': [100, 300],\n",
    "                  'criterion': ['gini', 'entropy']}\n",
    "gsc = model_selection.GridSearchCV(rfc, parameter_grid, cv=4, scoring='accuracy')\n",
    "gsc.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Once the grid search is done running, we can retreive the parameters that produced the best overall model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gsc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Likewise, we can get the best estimator (our machine learning model) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(\n",
    "    gsc.best_estimator_.predict(test_data), test_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "A `Pipeline` is a way to create a custom machine learning model from other Scikit-learn models. This can be useful when you want to apply multiple steps of data preparation (e.g. feature extraction or feature selection) and then run that prepared data through a classifier, regressor, or clustering algorithm. By combining multiple components into a pipeline, the entire custom model can be trained and evaluated with only a single method call, and can even be combined with `GridSearchCV` to optimize the methods used in the pipeline itself!\n",
    "\n",
    "With pipelines its easy to build very powerful composite machine learning models in only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this portion of the tutorial we will switch datasets to the Boston Housing dataset. The goal of using this data is to regress (predict a continuous value) the price of housing in Boston given 13 housing related variables. The attributes in this dataset are more independant than the previous dataset, so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e18a3bf0364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboston\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_boston\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDESCR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To create a pipeline, all that is needed is a list of components in the order that the data is exected to flow. This list is passed to the initializer of a Pipeline object, then the Pipeline object can be used just like any other Scikit-learn estimator.\n",
    "\n",
    "In this case, we will add a dimension reduction method to linear regression to take the number of attributes from 13 to 3, which shrinks the search space of the regressor and hopefully improves its performance. Principal Component Analysis (PCA) is, in essence, a method that creates a number of components that are linear combinations of each attribute in the original dataset with the constraint that each component is perpendicular to the others (in the linear algebra sense) and each component is sorted by the amount of variance it captures. The math is fairly heavy but you can read more about PCA [here](https://en.wikipedia.org/wiki/Principal_component_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=3)\n",
    "lr = linear_model.LinearRegression()\n",
    "pipe = [('dim_red', pca), ('lr', lr)]\n",
    "cpc = pipeline.Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Its also possible to create composite evalutation criterion. In this case we can make sure both mean squared error and r2 are used to evaluate our pipelined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'mse': metrics.make_scorer(metrics.mean_squared_error), \n",
    "           'r2': metrics.make_scorer(metrics.r2_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We can also directly cross-validate our pipelined model with the custom scoring criteria using the cross validate method. Notice we don't need to split the data into train and test sets when using cross validation because its handled by the model itself. However, this does not return a trained model, it is only used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.cross_validate(cpc, boston.data, boston.target, scoring=scoring, cv=3, return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Putting Everything Together\n",
    "Now that we have basic and ensemble methods, grid searches, CV, and pipelines in our toolkit, we can use them all at once to create a powerful composite estimator with optimized hyperparameters in only a handfull of lines of code. This last part will show how to use `GridSearchCV` to fill in a portion of our pipeline with the best estimator with the best parameters.\n",
    "\n",
    "The first step is to create a pipeline with a missing value for an estimator. We give it a name (dim_red for dimension reduction), but a value of None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "pipe = [('dim_red', None), ('lr', lr)] # None is a placeholder for a method\n",
    "cpr = pipeline.Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "For the grid search, we now add a new parameter to the grid that is the name of our missing module, and give it a list of estimator objects to choose from. Note that the `n_components` parameter is used by all 3 methods, so it can be optimized alongside choosing the actual method to use.\n",
    "\n",
    "The grid search has also been told to specifically use the mean squared error from our custom scoring criteria as the way to evaluate the pipeline, otherwise it does a poor job of fitting the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [\n",
    "    {\n",
    "        'dim_red': [decomposition.PCA(), decomposition.NMF(), decomposition.FactorAnalysis()],\n",
    "        'dim_red__n_components': [1,13]\n",
    "    }\n",
    "]\n",
    "cpgs = model_selection.GridSearchCV(cpr, parameter_grid, cv=3, scoring=scoring, refit='mse')\n",
    "cpgs.fit(boston.data, boston.target)\n",
    "cpgs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With a regressor which gives a continuous value, it can be harder to evaluate its performance compared to a classifier which has accuracy as well as precision/recall/f1. Mean squared error is not bounded to 0-1 like those measures are so its really only useful for comparing across models to see which gives the lowest score (and can be very decieving if the data is sinusoidal and the estimator predicts a straight line down the middle!). Usually the most sure way to check how the model is performing is to just visualize its output compared to the acutal value and to visually inspect how close the two line up. Of course this becomes trickier when there are multiple outputs, but since we have only one its straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the linear regressor itself to show improvement of dimension reduction\n",
    "lr.fit(boston.data, boston.target)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(boston.target)\n",
    "ax.plot(lr.predict(boston.data))\n",
    "ax.plot(cpgs.best_estimator_.predict(boston.data))\n",
    "ax.legend(['Actual', 'LR', 'Grid Search'])\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This data makes it a little difficult to visualize how well the models are matching the actual housing price, so we can sort the data in ascending-price order and see the trend much more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the indices of the prices by price\n",
    "inds = np.argsort(boston.target) \n",
    "# sort the data using the above indices\n",
    "data_sort = boston.data[inds]\n",
    "labels_sort = boston.target[inds]\n",
    "# perform same plotting as above but on sorted data/labels\n",
    "lr.fit(data_sort, labels_sort)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(labels_sort)\n",
    "ax.plot(lr.predict(data_sort))\n",
    "ax.plot(cpgs.best_estimator_.predict(data_sort))\n",
    "ax.legend(['Actual', 'LR', 'Grid Search'])\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
